{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-----+----+-----+------+------+-----+--------------+\n",
      "|  2696| 2697| 2698| 2699|2700| 2701|  2702|  2704|subid|      subgroup|\n",
      "+------+-----+-----+-----+----+-----+------+------+-----+--------------+\n",
      "| -50.0|-49.0|-48.0|-45.0|   N|train|A00001|1247.0|    2|train_subroup2|\n",
      "|  10.0|  9.0|  9.0|  8.0|   N|train|A00001|1247.0|    2|train_subroup2|\n",
      "|  46.0| 42.0| 36.0| 29.0|   N|train|A00001|1247.0|    2|train_subroup2|\n",
      "|   1.0|  4.0|  7.0| 10.0|   N| test|A00002|1248.0|    3| test_subroup3|\n",
      "|-112.0|-88.0|-66.0|-31.0|   N| test|A00002|1248.0|    3| test_subroup3|\n",
      "+------+-----+-----+-----+----+-----+------+------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, floor, concat, lit\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(str(i), DoubleType(), True) for i in range(2700)\n",
    "] + [\n",
    "    StructField('2700', StringType(), True),\n",
    "    StructField('2701', StringType(), True),\n",
    "    StructField('2702', StringType(), True)\n",
    "])\n",
    "\n",
    "path = \"./data/data_9s.csv\"\n",
    "df = spark.read.schema(schema).option(\"header\", True).csv(path)\n",
    "\n",
    "indexer = StringIndexer(inputCol='2702', outputCol='2704') \n",
    "indexed = indexer.fit(df).transform(df)\n",
    "df = indexed.withColumn(\"subid\", floor(col(\"2704\") % 5)).withColumn(\"subgroup\", concat(col(\"2701\"), lit(\"_subroup\"), col(\"subid\")))\n",
    "# show the last 10 columns of a piece of dataframe we are going to process\n",
    "df.select(df.columns[-10:]).show(5)\n",
    "df_org = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|subid|\n",
      "+-----+\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "|    3|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-------+-------------+\n",
      "|2700|indexed|      encoded|\n",
      "+----+-------+-------------+\n",
      "|   N|    0.0|(3,[0],[1.0])|\n",
      "|   N|    0.0|(3,[0],[1.0])|\n",
      "|   N|    0.0|(3,[0],[1.0])|\n",
      "|   N|    0.0|(3,[0],[1.0])|\n",
      "|   N|    0.0|(3,[0],[1.0])|\n",
      "+----+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|           spec_flat|\n",
      "+--------------------+\n",
      "|[1.0, 1.0, 1.0, 1...|\n",
      "|[1.0, 1.0, 1.0, 1...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Elapsed time is 15.892209 seconds.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pytictoc\n",
    "# using test as a speed comparison\n",
    "from pytictoc import TicToc\n",
    "import numpy as np\n",
    "import torch\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# loop through train / valid / test\n",
    "g = 'test'\n",
    "if not os.path.exists(g): \n",
    "    os.makedirs(g)\n",
    "df = df_org.filter((df_org['2701'] == g))\n",
    "\n",
    "\n",
    "t = TicToc() #create instance of class\n",
    "t.tic() #Start timer\n",
    "# engineer and save patient id information\n",
    "pid = df.select(df.columns[2704])\n",
    "pid.show(5)\n",
    "# pid.write.mode('overwrite').parquet(\"./\"+str(g)+\"/pid.parquet\")\n",
    "\n",
    "# engineer and save onehot outcome \n",
    "y = df.select(df.columns[2700])\n",
    "# StringIndexer step is necessary if the original column is categorical (non-numeric)\n",
    "string_indexer = StringIndexer(inputCol=y.columns[0], outputCol=\"indexed\")\n",
    "encoder = OneHotEncoder(inputCol=\"indexed\", outputCol=\"encoded\")\n",
    "# Define a pipeline with the required stages\n",
    "pipeline = Pipeline(stages=[string_indexer, encoder])\n",
    "# Fit the pipeline to the data and transform\n",
    "model = pipeline.fit(y)\n",
    "y = model.transform(y)\n",
    "# Show the result\n",
    "y.show(5)\n",
    "# y.write.mode('overwrite').parquet(\"./\"+str(g)+\"/y.parquet\")\n",
    "\n",
    "\n",
    "# engineer and convert 1d signal of each row of x into a 3d RGB spectrogram and flatten it in a list\n",
    "x = df.select(df.columns[:2700])\n",
    "# write a function to apply to each row of x\n",
    "def x2spec(row):\n",
    "    signal_data = list(row)\n",
    "\n",
    "    # Create the spectrogram\n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    ax.specgram(signal_data, Fs=300)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    canvas = FigureCanvasAgg(fig)\n",
    "    canvas.draw()\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    x_3d_spec_r = np.frombuffer(canvas.tostring_rgb(), dtype='uint8').reshape(int(height), int(width), 3)\n",
    "\n",
    "    # Convert to tensor, normalize and flatten\n",
    "    x_3d_spec_r = torch.Tensor(x_3d_spec_r).permute(2, 0, 1).unsqueeze(0)\n",
    "    x_3d_spec_r = x_3d_spec_r / 255.0\n",
    "    x_1d_spec_r = x_3d_spec_r.flatten()\n",
    "\n",
    "    # Convert the tensor to a list for PySpark compatibility\n",
    "    return [x_1d_spec_r.tolist()]\n",
    "\n",
    "# Apply the function to the RDD and convert to DataFrame\n",
    "x_rdd = x.rdd.map(x2spec)\n",
    "x_df = x_rdd.toDF([\"spec_flat\"])\n",
    "x_df.show(2)\n",
    "# x_df.write.mode('overwrite').parquet(\"./\"+str(g)+\"/x_spec_flat.parquet\")\n",
    "\n",
    "t.toc() #Time elapsed since t.tic() # Elapsed time is 15.892209 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 cores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
